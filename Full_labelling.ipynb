{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba011351",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79da34db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification, BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8058d4",
   "metadata": {},
   "source": [
    "# Jaccard Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff3b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the soft Jaccard loss for model training\n",
    "    :param y_true: Ground truth tensor (label-encoded).\n",
    "    :param y_pred: Prediction tensor (logits or probabilities).\n",
    "    :return: Jaccard loss.\n",
    "    \"\"\"\n",
    "    #get predicted probabilities for each class\n",
    "    y_pred = tf.nn.softmax(y_pred, axis=-1)\n",
    "    \n",
    "    #cast all the values to floats to ensure usability\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    #actual jaccard calculations\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=1)\n",
    "    union = tf.reduce_sum(y_true + y_pred, axis=1) - intersection\n",
    "    jaccard = (intersection) / (union +1e-10)\n",
    "    jaccard_loss = 1 - jaccard \n",
    "\n",
    "    return(jaccard_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8b27e3",
   "metadata": {},
   "source": [
    "# Mods and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceab5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here is where we load our best models and our unlablled data\n",
    "pos_mod = tf.keras.models.load_model('weight_pos_mod_2', custom_objects = {'jaccard_loss': jaccard_loss})\n",
    "neg_mod = tf.keras.models.load_model('weight_neg_mod', custom_objects = {'jaccard_loss': jaccard_loss})\n",
    "other_mod = tf.keras.models.load_model('weight_other_mod', custom_objects = {'jaccard_loss': jaccard_loss})\n",
    "unlab = pd.read_excel('Data.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e14919",
   "metadata": {},
   "source": [
    "# Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7015cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting_func(num_labs, full_df):\n",
    "    \"\"\"\n",
    "    Split datasets into positive, negative, and other sets, ignoring the already labelled data\n",
    "    :param num_labs: the number of responses in a given dataset\n",
    "    :param full_df: the datatset dataframe to be split up\n",
    "    :return: positive responses, negative responses, other responses, each in their own dataframes\n",
    "    \"\"\"\n",
    "    \n",
    "    #Positive Cleaning\n",
    "    pos_full = (full_df[['factors_would_attract_affiliate', 'factors_did_attract_affiliate']]).iloc[num_labs:]\n",
    "\n",
    "    #Negative Cleaning\n",
    "    neg_full = (full_df[['changes_keep', 'factors_would_deter_affiliate', 'drawbacks_affiliate', 'factors_prevent_satisfaction']]).iloc[num_labs:]\n",
    "\n",
    "\n",
    "    #Other Cleaning\n",
    "    other_full=(full_df[['factors_consider_during_eval','additional_comments']]).iloc[num_labs:]\n",
    "\n",
    "    \n",
    "    return(pos_full, neg_full, other_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135327ed",
   "metadata": {},
   "source": [
    "# Call to Cleaning, Splitting up into Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46573f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_full, neg_full, other_full = splitting_func(250, unlab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d910fddb",
   "metadata": {},
   "source": [
    "# Pre-Processing our Data for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c635b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bonus_func(column, tokenizer):\n",
    "    \"\"\"\n",
    "    Encode the datasets into ids and masks\n",
    "    :param column: the column of our dataset that we want to encode, should be applied to all columns\n",
    "    :param tokenizer: which tokenizer needs to be used to ensure the models receive necessary information\n",
    "    :return: input id array and attention mask array\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    #almost certain that this if else doesn't do anything anymore, but didn't want to remove in case it breaks something\n",
    "    #the block of code within the logic is definitely necessary though\n",
    "    for element in column:\n",
    "        if pd.isna(element):\n",
    "            encoded = tokenizer.encode_plus(element, max_length = 512, truncation = True, padding='max_length', return_attention_mask = True)\n",
    "            input_ids.append(encoded['input_ids'])\n",
    "            attention_masks.append(encoded['attention_mask'])\n",
    "        \n",
    "        else:\n",
    "            encoded = tokenizer.encode_plus(element, max_length = 512, truncation = True, padding='max_length', return_attention_mask = True)\n",
    "            input_ids.append(encoded['input_ids'])\n",
    "            attention_masks.append(encoded['attention_mask'])\n",
    "\n",
    "            \n",
    "\n",
    "                \n",
    "               \n",
    "    input_ids=np.array(input_ids)\n",
    "    attention_masks =np.array(attention_masks)\n",
    "    return(input_ids, attention_masks)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a7afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_proc(model, df):\n",
    "    \"\"\"\n",
    "    Create necessary information regarding our models and call the bonus function to encode our data\n",
    "    :param model: name of the model being used currently\n",
    "    :param df: the dataset we want encoded in that model's fashion\n",
    "    :return: encoded text data, including the ids and corresponding masks\n",
    "    \"\"\"\n",
    "    if model == \"BERT\":\n",
    "        model_name= \"bert-base-uncased\"\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)       \n",
    " \n",
    "    elif model == \"DISTILBERT\":\n",
    "        model_name= \"distilbert-base-uncased\"\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "\n",
    "    for column in df:\n",
    "\n",
    "        input_ids2, attention_masks2 = bonus_func(df[column], tokenizer)\n",
    "\n",
    "        input_ids.append(input_ids2)\n",
    "        attention_masks.append(attention_masks2)\n",
    "\n",
    "    return(input_ids, attention_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2219cb",
   "metadata": {},
   "source": [
    "# Calls to Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a106f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling our pre-processing. This may need to be expanded a touch if not using the same model for every instantiation of a category\n",
    "pos_ids, pos_masks = pre_proc('DISTILBERT',pos_full)\n",
    "neg_ids, neg_masks= pre_proc('DISTILBERT', neg_full)\n",
    "other_ids, other_masks = pre_proc('BERT', other_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efc4b95",
   "metadata": {},
   "source": [
    "# Lets Get a Lil Funky (necessary comment at the top of the first block in this section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c967a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is where we have the models make predictions on the data. BERT models require the token type ids in order to function\n",
    "#DistilBERT models do not. Each Category has a block here,  doing the same thing as this one. \n",
    "#Results are dumped into a pickle file for each category, containing a dictionary of model outputs for each question\n",
    "pos_ids1 = pos_ids[0]\n",
    "pos_masks1=pos_masks[0]\n",
    "tensor_input_ids = tf.convert_to_tensor(pos_ids1)\n",
    "tensor_attention_masks = tf.convert_to_tensor(pos_masks1)\n",
    "tensor_token_type_ids = tf.zeros_like(tensor_input_ids, dtype=tf.int32)\n",
    "inputs = {\n",
    "    'input_ids': tensor_input_ids,\n",
    "    'attention_mask': tensor_attention_masks,\n",
    "#    'token_type_ids': tensor_token_type_ids\n",
    "}\n",
    "\n",
    "pos_output1 = pos_mod.predict(inputs)\n",
    "pos_ids2 = pos_ids[1]\n",
    "pos_masks2=pos_masks[1]\n",
    "tensor_input_ids = tf.convert_to_tensor(pos_ids2)\n",
    "tensor_attention_masks = tf.convert_to_tensor(pos_masks2)\n",
    "tensor_token_type_ids = tf.zeros_like(tensor_input_ids, dtype=tf.int32)\n",
    "inputs = {\n",
    "    'input_ids': tensor_input_ids,\n",
    "    'attention_mask': tensor_attention_masks,\n",
    "#    'token_type_ids': tensor_token_type_ids\n",
    "}\n",
    "\n",
    "pos_output2 = pos_mod.predict(inputs)\n",
    "\n",
    "prefix = 'dict2_'\n",
    "pos_output2 = {prefix + key: value for key, value in pos_output2.items()}\n",
    "\n",
    "combined_dict = {**pos_output1, **pos_output2}\n",
    "with open('best_pos_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(combined_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622f1294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b6be65",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_ids1 = neg_ids[0]\n",
    "neg_masks1=neg_masks[0]\n",
    "tensor_input_ids = tf.convert_to_tensor(neg_ids1)\n",
    "tensor_attention_masks = tf.convert_to_tensor(neg_masks1)\n",
    "#tensor_token_type_ids = tf.zeros_like(tensor_input_ids, dtype=tf.int32)\n",
    "inputs = {\n",
    "    'input_ids': tensor_input_ids,\n",
    "    'attention_mask': tensor_attention_masks,\n",
    "#    'token_type_ids': tensor_token_type_ids\n",
    "}\n",
    "\n",
    "neg_output1 = neg_mod.predict(inputs)\n",
    "\n",
    "neg_ids2 = neg_ids[1]\n",
    "neg_masks2=neg_masks[1]\n",
    "tensor_input_ids = tf.convert_to_tensor(neg_ids2)\n",
    "tensor_attention_masks = tf.convert_to_tensor(neg_masks2)\n",
    "#tensor_token_type_ids = tf.zeros_like(tensor_input_ids, dtype=tf.int32)\n",
    "inputs = {\n",
    "    'input_ids': tensor_input_ids,\n",
    "    'attention_mask': tensor_attention_masks,\n",
    "#    'token_type_ids': tensor_token_type_ids\n",
    "}\n",
    "\n",
    "neg_output2 = neg_mod.predict(inputs)\n",
    "prefix = 'dict2_'\n",
    "neg_output2 = {prefix + key: value for key, value in neg_output2.items()}\n",
    "\n",
    "\n",
    "neg_ids3 = neg_ids[2]\n",
    "neg_masks3=neg_masks[2]\n",
    "tensor_input_ids = tf.convert_to_tensor(neg_ids3)\n",
    "tensor_attention_masks = tf.convert_to_tensor(neg_masks3)\n",
    "#tensor_token_type_ids = tf.zeros_like(tensor_input_ids, dtype=tf.int32)\n",
    "inputs = {\n",
    "    'input_ids': tensor_input_ids,\n",
    "    'attention_mask': tensor_attention_masks,\n",
    "#    'token_type_ids': tensor_token_type_ids\n",
    "}\n",
    "\n",
    "neg_output3 = neg_mod.predict(inputs)\n",
    "prefix = 'dict3_'\n",
    "neg_output3 = {prefix + key: value for key, value in neg_output3.items()}\n",
    "\n",
    "\n",
    "\n",
    "neg_ids4 = neg_ids[3]\n",
    "neg_masks4=neg_masks[3]\n",
    "tensor_input_ids = tf.convert_to_tensor(neg_ids4)\n",
    "tensor_attention_masks = tf.convert_to_tensor(neg_masks4)\n",
    "#tensor_token_type_ids = tf.zeros_like(tensor_input_ids, dtype=tf.int32)\n",
    "inputs = {\n",
    "    'input_ids': tensor_input_ids,\n",
    "    'attention_mask': tensor_attention_masks,\n",
    "#    'token_type_ids': tensor_token_type_ids\n",
    "}\n",
    "\n",
    "neg_output4 = neg_mod.predict(inputs)\n",
    "\n",
    "prefix = 'dict4_'\n",
    "neg_output4 = {prefix + key: value for key, value in neg_output4.items()}\n",
    "\n",
    "\n",
    "combined_dict = {**neg_output1, **neg_output2, **neg_output3, **neg_output4}\n",
    "with open('best_neg_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(combined_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4996c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_ids1 = other_ids[0]\n",
    "other_masks1=other_masks[0]\n",
    "tensor_input_ids = tf.convert_to_tensor(other_ids1)\n",
    "tensor_attention_masks = tf.convert_to_tensor(other_masks1)\n",
    "tensor_token_type_ids = tf.zeros_like(tensor_input_ids, dtype=tf.int32)\n",
    "inputs = {\n",
    "    'input_ids': tensor_input_ids,\n",
    "    'attention_mask': tensor_attention_masks,\n",
    "    'token_type_ids': tensor_token_type_ids\n",
    "}\n",
    "\n",
    "other_output1 = other_mod.predict(inputs)\n",
    "other_ids2 = other_ids[1]\n",
    "other_masks2=other_masks[1]\n",
    "tensor_input_ids = tf.convert_to_tensor(other_ids2)\n",
    "tensor_attention_masks = tf.convert_to_tensor(other_masks2)\n",
    "tensor_token_type_ids = tf.zeros_like(tensor_input_ids, dtype=tf.int32)\n",
    "inputs = {\n",
    "    'input_ids': tensor_input_ids,\n",
    "    'attention_mask': tensor_attention_masks,\n",
    "    'token_type_ids': tensor_token_type_ids\n",
    "}\n",
    "\n",
    "other_output2 = other_mod.predict(inputs)\n",
    "prefix = 'dict2_'\n",
    "other_output2 = {prefix + key: value for key, value in other_output2.items()}\n",
    "\n",
    "\n",
    "combined_dict = {**other_output1, **other_output2}\n",
    "with open('best_other_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(combined_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190efa0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff34386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading in our full dataset fresh as well as the pickle files created above\n",
    "test = pd.read_excel('Data.xlsx')\n",
    "pos_files = ['best_pos_dict.pkl']\n",
    "neg_files =  ['best_neg_dict.pkl']\n",
    "other_files = ['best_other_dict.pkl']\n",
    "\n",
    "#loops through the positive pickle file, pulling each question's individual dictionary out, determining predictions\n",
    "#and appending them to our full dataset\n",
    "i = 0\n",
    "print(\"POSITIVE\")\n",
    "for dat in pos_files:\n",
    "    with open(dat, 'rb') as file:\n",
    "        cur_dict = pickle.load(file)\n",
    "        for key in cur_dict.keys():\n",
    "            logs = cur_dict[key]\n",
    "            probs = tf.nn.softmax(logs, axis = -1)\n",
    "            mask = probs>.13  #our positive cutoff found through testing\n",
    "            inds_above= tf.where(mask)\n",
    "            preds_df = pd.DataFrame(inds_above)\n",
    "            preds_df.columns = ['response_index', 'predicted_label']\n",
    "            grouped = preds_df.groupby('response_index')['predicted_label'].agg(list).reset_index()\n",
    "            lab_name= \"pos_preds\" + str(i)\n",
    "            test[lab_name] = grouped['predicted_label']\n",
    "            i = i+1\n",
    "#loops through the negative pickle file, pulling each question's individual dictionary out, determining predictions\n",
    "#and appending them to our full dataset     \n",
    "i=0       \n",
    "print(\"NEGATIVE\")\n",
    "for dat in neg_files:\n",
    "    with open(dat, 'rb') as file:\n",
    "        cur_dict = pickle.load(file)\n",
    "        for key in cur_dict.keys():\n",
    "            logs = cur_dict[key]\n",
    "            probs = tf.nn.softmax(logs, axis = -1)\n",
    "            mask = probs>.30 #our negative cutoff found through testing\n",
    "            inds_above= tf.where(mask)\n",
    "            preds_df = pd.DataFrame(inds_above)\n",
    "            preds_df.columns = ['response_index', 'predicted_label']\n",
    "            grouped = preds_df.groupby('response_index')['predicted_label'].agg(list).reset_index()\n",
    "            lab_name= \"neg_preds\" + str(i)\n",
    "            test[lab_name] = grouped['predicted_label']\n",
    "            i = i+1\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "#loops through the other pickle file, pulling each question's individual dictionary out, determining predictions\n",
    "#and appending them to our full dataset\n",
    "i=0\n",
    "print(\"OTHER\")\n",
    "for dat in other_files:\n",
    "    with open(dat, 'rb') as file:\n",
    "        cur_dict = pickle.load(file)\n",
    "        for key in cur_dict.keys():\n",
    "            logs = cur_dict[key]\n",
    "            probs = tf.nn.softmax(logs, axis = -1)\n",
    "            mask = probs>.25 #our other cutoff found through testing\n",
    "            inds_above= tf.where(mask)\n",
    "\n",
    "            preds_df = pd.DataFrame(inds_above)\n",
    "            #fixing some specific responses right here, will likely need to change\n",
    "            preds_df.columns = ['response_index', 'predicted_label']\n",
    "            if key == 'logits':\n",
    "                new_row3 = {'response_index': 3483, 'predicted_label': 0}\n",
    "                new_df = pd.DataFrame([new_row3])\n",
    "                preds_df = pd.concat([preds_df, new_df], ignore_index=True)\n",
    "            elif key== 'dict2_logits':\n",
    "                new_row1 = {'response_index': 1196, 'predicted_label': 0}\n",
    "                new_row2 = {'response_index': 3454, 'predicted_label': 0}\n",
    "                \n",
    "                \n",
    "                new_df = pd.DataFrame([new_row1])\n",
    "                preds_df = pd.concat([preds_df, new_df], ignore_index=True)\n",
    "\n",
    "                new_df = pd.DataFrame([new_row2])\n",
    "                preds_df = pd.concat([preds_df, new_df], ignore_index=True)\n",
    "            #organizing all the data\n",
    "            preds_df = preds_df.sort_values(by='response_index')\n",
    "            preds_df = preds_df.reset_index(drop=True)\n",
    "            grouped = preds_df.groupby('response_index')['predicted_label'].agg(list).reset_index()\n",
    "            print(grouped)\n",
    "            lab_name= \"other_preds\" + str(i)\n",
    "            test[lab_name] = grouped['predicted_label']\n",
    "            i = i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ae3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sends labelled data to a new excel file\n",
    "test.to_excel('full_obs.xlsx', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
