{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598a364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4661e0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#currently shows for the three class case. Can be changed by removing the \"Undecided\" class name and mapping all\n",
    "# -1's to 0's\n",
    "#This one is slightly different than the others due to how XGBoost runs\n",
    "class_names = [\"Undecided\", \"Staying\", \"Not Staying\"]\n",
    "X_train= pd.read_excel('X_train.xlsx')\n",
    "X_test=pd.read_excel('X_test.xlsx')\n",
    "X_val=pd.read_excel('X_val.xlsx')\n",
    "y_train=pd.read_excel('y_train.xlsx')\n",
    "y_test=pd.read_excel('y_test.xlsx')\n",
    "y_val=pd.read_excel('y_val.xlsx')\n",
    "\n",
    "#This mapping is necessary because XGBoost doesnt like the negative class label\n",
    "y_train.replace(-1, 2, inplace=True)\n",
    "y_test.replace(-1, 2, inplace=True)\n",
    "y_val.replace(-1, 2, inplace=True)\n",
    "\n",
    "#y dataframes ending in 0 are for predicting post-ADSC retention\n",
    "#y dataframes ending in 1 are for predicting retention until retirement\n",
    "y_train0 = y_train['intention_beyond_commitment']\n",
    "y_train1 = y_train['intention_toward_retirement']\n",
    "y_val0 = y_val['intention_beyond_commitment']\n",
    "y_val1 = y_val['intention_toward_retirement']\n",
    "y_test0 = y_test['intention_beyond_commitment']\n",
    "y_test1 = y_test['intention_toward_retirement']\n",
    "top_n = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ef1b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing our data based on the distribution of the training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = pd.DataFrame(scaler.transform(X_train), columns = X_train.columns)\n",
    "X_val = pd.DataFrame(scaler.transform(X_val), columns = X_val.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177e97bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training an XGBoost model for post-ADSC Retention\n",
    "mod0 = xgb.XGBClassifier(n_estimators = 20, max_depth = 15)\n",
    "mod0.fit(X_train, y_train0)\n",
    "y_pred = mod0.predict(X_val)\n",
    "accuracy = accuracy_score(y_val0, y_pred)\n",
    "average_type = 'macro'  # Can be 'micro', 'macro', or 'weighted'\n",
    "precision_avg, recall_avg, f1_avg, _ = precision_recall_fscore_support(y_val0, y_pred, average=average_type)\n",
    "\n",
    "print(f\"\\nOverall Accuracy ({average_type}): \", accuracy)\n",
    "print(f\"\\nOverall Precision ({average_type}): \", precision_avg)\n",
    "print(f\"Overall Recall ({average_type}): \", recall_avg)\n",
    "print(f\"Overall F1 Score ({average_type}): \", f1_avg)\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_val0, y_pred)\n",
    "\n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Intention Toward Staying After Committment')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "feature_importance = mod0.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importance})\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the sorted feature importance\n",
    "print(feature_importance_df)\n",
    "\n",
    "feature_importances= mod0.feature_importances_\n",
    "# Sort feature importances in descending order\n",
    "sorted_importances = np.sort(feature_importances)[::-1]\n",
    "\n",
    "# Calculate cumulative sum\n",
    "cumulative_importance = np.cumsum(sorted_importances)\n",
    "\n",
    "# Plot the cumulative importance\n",
    "plt.plot(range(1, len(cumulative_importance) + 1), cumulative_importance)\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Cumulative Importance')\n",
    "plt.title('Cumulative Feature Importance')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Select the top 100 Features, this was done by eyeing the chart\n",
    "selected_features = feature_importance_df['Feature'][:100].tolist()\n",
    "X_train0 = X_train[selected_features]\n",
    "X_val0 = X_val[selected_features]\n",
    "X_test0 = X_test[selected_features]\n",
    "\n",
    "#New XGBoost model using only the features selected in the first model on validation data\n",
    "mod0fs = xgb.XGBClassifier(n_estimators = 20, max_depth = 15)\n",
    "mod0fs.fit(X_train0, y_train0)\n",
    "\n",
    "\n",
    "y_pred = mod0fs.predict(X_val0)\n",
    "accuracy = accuracy_score(y_val0, y_pred)\n",
    "precision_avg, recall_avg, f1_avg, _ = precision_recall_fscore_support(y_val0, y_pred, average=average_type)\n",
    "print(f\"\\nOverall Accuracy ({average_type}): \", accuracy)\n",
    "print(f\"\\nOverall Precision ({average_type}): \", precision_avg)\n",
    "print(f\"Overall Recall ({average_type}): \", recall_avg)\n",
    "print(f\"Overall F1 Score ({average_type}): \", f1_avg)\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_val0, y_pred)\n",
    "\n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Intention Toward Staying After Committment')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "#Using our model on the test set as well\n",
    "y_pred = mod0fs.predict(X_test0)\n",
    "accuracy = accuracy_score(y_test0, y_pred)\n",
    "report = classification_report(y_test0, y_pred)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "precision_avg, recall_avg, f1_avg, _ = precision_recall_fscore_support(y_test0, y_pred, average=average_type)\n",
    "\n",
    "print(f\"\\nOverall Precision ({average_type}): \", precision_avg)\n",
    "print(f\"Overall Recall ({average_type}): \", recall_avg)\n",
    "print(f\"Overall F1 Score ({average_type}): \", f1_avg)\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test0, y_pred)\n",
    "\n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Intention Toward Staying After Committment')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "feature_importances= mod0fs.feature_importances_\n",
    "feature_names = X_train0.columns.tolist()\n",
    "sorted_idx = feature_importances.argsort()[::-1]\n",
    "\n",
    "# Get the top 20 most important features\n",
    "\n",
    "top_features = [feature_names[i] for i in sorted_idx[:top_n]]\n",
    "top_importances = [feature_importances[i] for i in sorted_idx[:top_n]]\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(top_n), top_importances, align=\"center\")\n",
    "plt.yticks(range(top_n), top_features)\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title('Top 20 Feature Importances in Predicting Post-ADSC \\nRetention Utilising XGBoost')\n",
    "#plt.show()\n",
    "plt.tight_layout()\n",
    "plt.savefig('ADSC_XGB_with_und.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983f1cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training an XGBoost model for retention until retirement\n",
    "mod1 = xgb.XGBClassifier(n_estimators = 20, max_depth = 20)\n",
    "mod1.fit(X_train, y_train1)\n",
    "y_pred = mod1.predict(X_val)\n",
    "accuracy = accuracy_score(y_val1, y_pred)\n",
    "report = classification_report(y_val1, y_pred)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "precision_avg, recall_avg, f1_avg, _ = precision_recall_fscore_support(y_val1, y_pred, average=average_type)\n",
    "\n",
    "print(f\"\\nOverall Precision ({average_type}): \", precision_avg)\n",
    "print(f\"Overall Recall ({average_type}): \", recall_avg)\n",
    "print(f\"Overall F1 Score ({average_type}): \", f1_avg)\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_val1, y_pred)\n",
    "\n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Intention Toward Staying After Committment')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "feature_importance = mod1.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importance})\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the sorted feature importance\n",
    "print(feature_importance_df)\n",
    "\n",
    "feature_importances= mod1.feature_importances_\n",
    "# Sort feature importances in descending order\n",
    "sorted_importances = np.sort(feature_importances)[::-1]\n",
    "\n",
    "# Calculate cumulative sum\n",
    "cumulative_importance = np.cumsum(sorted_importances)\n",
    "\n",
    "# Plot the cumulative importance\n",
    "plt.plot(range(1, len(cumulative_importance) + 1), cumulative_importance)\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Cumulative Importance')\n",
    "plt.title('Cumulative Feature Importance')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Select the top 110 features, this was done by eyeing the chart\n",
    "selected_features = feature_importance_df['Feature'][:110].tolist()\n",
    "X_train1 = X_train[selected_features]\n",
    "X_val1 = X_val[selected_features]\n",
    "X_test1 = X_test[selected_features]\n",
    "\n",
    "#New XGBoost model using only the features selected in the first model on validation data\n",
    "mod1fs = xgb.XGBClassifier(n_estimators = 20, max_depth = 20)\n",
    "mod1fs.fit(X_train1, y_train1)\n",
    "\n",
    "\n",
    "y_pred = mod1fs.predict(X_val1)\n",
    "accuracy = accuracy_score(y_val1, y_pred)\n",
    "report = classification_report(y_val1, y_pred)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "precision_avg, recall_avg, f1_avg, _ = precision_recall_fscore_support(y_val1, y_pred, average=average_type)\n",
    "\n",
    "print(f\"\\nOverall Precision ({average_type}): \", precision_avg)\n",
    "print(f\"Overall Recall ({average_type}): \", recall_avg)\n",
    "print(f\"Overall F1 Score ({average_type}): \", f1_avg)\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_val1, y_pred)\n",
    "\n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Intention Toward Staying After Committment')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Using our model on the test set as well\n",
    "y_pred = mod1fs.predict(X_test1)\n",
    "accuracy = accuracy_score(y_test1, y_pred)\n",
    "report = classification_report(y_test1, y_pred)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "precision_avg, recall_avg, f1_avg, _ = precision_recall_fscore_support(y_test1, y_pred, average=average_type)\n",
    "\n",
    "print(f\"\\nOverall Precision ({average_type}): \", precision_avg)\n",
    "print(f\"Overall Recall ({average_type}): \", recall_avg)\n",
    "print(f\"Overall F1 Score ({average_type}): \", f1_avg)\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test1, y_pred)\n",
    "\n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Intention Toward Staying After Committment')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "feature_importances= mod1fs.feature_importances_\n",
    "feature_names = X_train1.columns.tolist()\n",
    "sorted_idx = feature_importances.argsort()[::-1]\n",
    "\n",
    "# Get the top N most important features (change N to the desired number)\n",
    "\n",
    "top_features = [feature_names[i] for i in sorted_idx[:top_n]]\n",
    "top_importances = [feature_importances[i] for i in sorted_idx[:top_n]]\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(top_n), top_importances, align=\"center\")\n",
    "plt.yticks(range(top_n), top_features)\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title('Top 20 Feature Importances in Predicting Retention \\nUntil Retirement Utilising XGBoost')\n",
    "#plt.show()\n",
    "plt.tight_layout()\n",
    "plt.savefig('retirement_XGB_with_und.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
