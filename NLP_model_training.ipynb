{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a617ca44",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5786d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification, BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import re\n",
    "import string\n",
    "import transformers\n",
    "import sklearn\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db25b14b",
   "metadata": {},
   "source": [
    "# Data Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee78c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset split already performed in NLP Set Splitter\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "val = pd.read_csv('val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d016a71e",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9738ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, lower= False, remove_numbers= True):\n",
    "    \"\"\"\n",
    "    Convert raw text to text that can be better utilized by our encoder\n",
    "    :param text: Raw text data from survey \n",
    "    :param lower: Whether to convert all text to lower-case\n",
    "    :param remove_numbers: Whether to delete all numeric values from the text\n",
    "    :return: modified text to be fed into encoder\n",
    "    \"\"\"\n",
    "    #convert to lowercase if desired\n",
    "    if lower:\n",
    "        text= text.lower()\n",
    "        \n",
    "    #remove garbage characters\n",
    "    text = re.sub(r'[^\\w\\s]', '',text)\n",
    "    \n",
    "    #remove numbers if desired\n",
    "    if remove_numbers:\n",
    "        text= re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r']\\s+', ' ', text).strip()\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2daa10",
   "metadata": {},
   "source": [
    "# Jaccard Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e80d64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the soft Jaccard loss for model training\n",
    "    :param y_true: Ground truth tensor (label-encoded).\n",
    "    :param y_pred: Prediction tensor (logits or probabilities).\n",
    "    :return: Jaccard loss.\n",
    "    \"\"\"\n",
    "    #get predicted probabilities for each class\n",
    "    y_pred = tf.nn.softmax(y_pred, axis=-1)\n",
    "    #cast all the values to floats to ensure usability\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    #actual jaccard calculations\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=1)\n",
    "    union = tf.reduce_sum(y_true + y_pred, axis=1) - intersection\n",
    "    jaccard = (intersection) / (union +1e-10)\n",
    "    jaccard_loss = 1 - jaccard \n",
    "\n",
    "    return(jaccard_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09b813c",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5382b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mods(train, valid, num_classes, learning_rate, opt, model, class_weights):\n",
    "    \"\"\"\n",
    "    Perform downstream training of BERT or DistilBERT using selected hyperparameters\n",
    "    :param train: set of training data, including text and labels\n",
    "    :param valid: set of validation data, including text and labels\n",
    "    :param learning_rate: learning rate to be used by the optimizer for training\n",
    "    :param opt:optimizer to be used for training\n",
    "    :param model: NLP model to be used (BERT or DistilBERT)\n",
    "    :param class_weights: dictionary of class weights used in training\n",
    "    :return: model history, numerical representation of validation text data, \n",
    "       one-hot encoded matrix of validation labels, and a trained model\n",
    "    \"\"\"\n",
    "    #create model instantiations based off what was passed in\n",
    "    if model == \"BERT\":\n",
    "        model_name= \"bert-base-uncased\"\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        mod = TFBertForSequenceClassification.from_pretrained(model_name, num_labels = num_classes)\n",
    "    elif model == \"DISTILBERT\":\n",
    "        model_name= \"distilbert-base-uncased\"\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "        mod = TFDistilBertForSequenceClassification.from_pretrained(model_name, num_labels = num_classes)\n",
    "    \n",
    "    \n",
    "    #initializing empty arrays\n",
    "    train_input_ids = []\n",
    "    train_attention_masks = []\n",
    "\n",
    "    #clean each response and seperate the numerical representations from their masks\n",
    "    for sentence in train['text']:\n",
    "        cleaned = clean_text(sentence)\n",
    "        encoded = tokenizer.encode_plus(cleaned, max_length = 512, truncation = True, padding='max_length', return_attention_mask = True)\n",
    "        train_input_ids.append(encoded['input_ids'])\n",
    "        train_attention_masks.append(encoded['attention_mask'])\n",
    "    \n",
    "    #transforming the data into a more usable form\n",
    "    train_input_ids=np.array(train_input_ids)\n",
    "    train_attention_masks =np.array(train_attention_masks)\n",
    "\n",
    "    \n",
    "    #one hot encoding our labels\n",
    "    train_labels = train['labels']\n",
    "\n",
    "    train_labels = [list(map(int, str(item).split(','))) for item in train_labels]\n",
    "    train_labels = pad_sequences(train_labels, maxlen=num_classes)\n",
    "    train_labels = np.array(train_labels)\n",
    "    \n",
    "    result = np.zeros(train_labels.shape, dtype=int)\n",
    "\n",
    "    for idx, val in enumerate(range(1, num_classes)):\n",
    "        rows_with_val = np.where(train_labels == val)[0]\n",
    "        for row in rows_with_val:\n",
    "            result[row, idx+1] = 1\n",
    "\n",
    "    y_train = result\n",
    "    \n",
    "    #this section repeats the same thing we just did for the training data, just for the validation stuff\n",
    "    val_input_ids = []\n",
    "    val_attention_masks = []\n",
    "\n",
    "    for sentence in valid['text']:\n",
    "        cleaned = clean_text(sentence)\n",
    "        encoded = tokenizer.encode_plus(cleaned, max_length = 512, truncation = True, padding='max_length', return_attention_mask = True)\n",
    "        val_input_ids.append(encoded['input_ids'])\n",
    "        val_attention_masks.append(encoded['attention_mask'])\n",
    "    \n",
    "    val_input_ids=np.array(val_input_ids)\n",
    "    val_attention_masks =np.array(val_attention_masks)\n",
    "    \n",
    "    val_labels = valid['labels']\n",
    "   \n",
    "    val_labels = [list(map(int, str(item).split(','))) for item in val_labels]\n",
    "    val_labels = pad_sequences(val_labels, maxlen=num_classes)\n",
    "    val_labels = np.array(val_labels)\n",
    "    \n",
    "    result = np.zeros(val_labels.shape, dtype=int)\n",
    "\n",
    "    for idx, val in enumerate(range(1, num_classes)):\n",
    "        rows_with_val = np.where(val_labels == val)[0]\n",
    "        for row in rows_with_val:\n",
    "            result[row, idx+1] = 1\n",
    "            \n",
    "    #renaming some variables to make them read easier\n",
    "    y_val = result\n",
    "    \n",
    "    \n",
    "    x_val = val_input_ids\n",
    "    mask_val = val_attention_masks\n",
    "    \n",
    "    x_train = train_input_ids\n",
    "    mask_train = train_attention_masks\n",
    "    \n",
    "    #creating our optimizer based on passed in parameters\n",
    "    if opt == \"ADAM\":\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate =learning_rate)\n",
    "    elif opt == \"SGD\":\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate =learning_rate)\n",
    "        \n",
    "    elif opt == \"RMSPROP\":\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate =learning_rate)\n",
    "    mod.compile(optimizer = optimizer, loss =jaccard_loss)\n",
    "    \n",
    "    #early stopping in case the model just really isn't working\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 20)\n",
    "    \n",
    "    #actual training step\n",
    "    history = mod.fit([x_train, mask_train], y_train, batch_size =8, validation_data = ([x_val, mask_val], y_val) , callbacks = [early_stopping], epochs= 250, class_weight = class_weights)\n",
    "    return(history, x_val, y_val, mask_val, mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8167fc3d",
   "metadata": {},
   "source": [
    "# Jaccard Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2093e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(setA, setB):\n",
    "    \"\"\"\n",
    "    Compute the Jaccard similarity for two sets of labels\n",
    "    :param setA: set of true labels\n",
    "    :param setB: set of predicted labels\n",
    "    :return: Jaccard Similarity Score\n",
    "    \"\"\"\n",
    "    #making the sets usable\n",
    "    setA = set(map(int, setA.strip(\"{}\").split(',')))\n",
    "    setB = set(map(lambda x: int(float(x)), setB.strip(\"{}\").split(',')))\n",
    "    \n",
    "    #jaccard calculations\n",
    "    intersection = len(setA.intersection(setB))\n",
    "    intersection1 = setA.intersection(setB)\n",
    "    union = len(setA.union(setB))\n",
    "    return intersection/union if union != 0 else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbfa5c4",
   "metadata": {},
   "source": [
    "# Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdd2548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_testing(mod, x_test, y_test, mask_test, num_classes):\n",
    "    \"\"\"\n",
    "    Check how the models are performing on validation data or test data\n",
    "    :param mod: the tensorflow model we're interested in testing  \n",
    "    :param x_test: numerical representation of text data we're testing\n",
    "    :param y_test: one-hot encoded matrix of true labels\n",
    "    :param mask_test: encoded masks of the text data, necessary for BERT to operate\n",
    "    :param num_classes: number of factors used in that category during hand-labelling\n",
    "    :return: list of Jaccard Similarity Scores for each possible level of acception for predictions\n",
    "    \"\"\"\n",
    "    #find probabilities of outputs\n",
    "    outputs =mod(x_test, attention_mask = mask_test)\n",
    "    logits = outputs.logits\n",
    "    probs = tf.nn.softmax(logits, axis = -1)\n",
    "    \n",
    "    #creating a range of values to make this display a bit better\n",
    "    vals = [i/100 for i in range (1,100)]\n",
    "    avg_jac=[]\n",
    "    for i in vals:\n",
    "        mask = probs > i\n",
    "        inds_above = tf.where(mask)\n",
    "        probs_above = tf.gather_nd(probs, inds_above)\n",
    "        if len(inds_above > 0):\n",
    "\n",
    "            #creating dataframes to do our matching\n",
    "            pred_check = pd.DataFrame(y_test)\n",
    "\n",
    "            preds_df = pd.DataFrame(inds_above)\n",
    "            preds_df.columns = ['response_index', 'predicted_label']\n",
    "\n",
    "            #some labelling\n",
    "            pred_check['response_index'] = range(len(y_test))\n",
    "\n",
    "            #join tables to match predictions with initial labels\n",
    "            left_join = pred_check.merge(preds_df, on = 'response_index', how = 'left')\n",
    "            left_join.fillna(0, inplace=True)\n",
    "\n",
    "            #joins each individual response's predictions into one\n",
    "            left_join = left_join.groupby('response_index', as_index = False).agg(lambda x: ','.join(map(str, x)))\n",
    "\n",
    "            def deduplicate_values(s):\n",
    "                values = s.split(',')\n",
    "                unique_values = list(set(values))\n",
    "                return ','.join(unique_values)\n",
    "\n",
    "            def combine_values(row):\n",
    "                return ','.join(row)\n",
    "\n",
    "            # Apply the function to our columns\n",
    "            for col in left_join.columns[1:(num_classes + 1)]:\n",
    "                left_join[col] = left_join[col].apply(deduplicate_values)\n",
    "            left_join['true_labels'] = left_join.iloc[:, 1:num_classes+1].apply(combine_values, axis=1)\n",
    "            left_join = left_join.drop(columns=left_join.columns[1:(num_classes + 1)])\n",
    "\n",
    "            def non_zero_indices(s):\n",
    "                values = s.split(',')\n",
    "                indices = [str(i) for i, v in enumerate(values) if v != '0']\n",
    "                return ','.join(indices)\n",
    "            left_join['true_labels'] = left_join['true_labels'].apply(non_zero_indices)\n",
    "            \n",
    "            left_join['jaccard'] = left_join.apply(lambda row: jaccard_similarity(row['true_labels'], row['predicted_label']), axis = 1)\n",
    "           \n",
    "            avg = left_join['jaccard'].mean()\n",
    "            avg_jac.append(avg)\n",
    "    return(avg_jac)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1babe1",
   "metadata": {},
   "source": [
    "# Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913e7295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_func(num_labs, full_df):\n",
    "    \"\"\"\n",
    "    Split datasets into positive, negative, and other sets. Also removes non-responses\n",
    "    :param num_labs: the number of responses in a given dataset\n",
    "    :param full_df: the datatset dataframe to be split up\n",
    "    :return: positive responses, negative responses, other responses, each in their own dataframes with corresponding labels\n",
    "    \"\"\"\n",
    "    #Positive Cleaning, creating the positive set and dropping non-responses\n",
    "    nlp_train_pos = (full_df[['factors_would_attract_affiliate', 'factors_would_attract_affiliate_labels','factors_did_attract_affiliate','factors_did_attract_affiliate_labels']]).iloc[0:num_labs]\n",
    "\n",
    "    fwaa = nlp_train_pos[['factors_would_attract_affiliate', 'factors_would_attract_affiliate_labels']]\n",
    "    fwaa = fwaa.explode('factors_would_attract_affiliate_labels', ignore_index=True)\n",
    "    fwaa = fwaa[fwaa['factors_would_attract_affiliate_labels'] !='0']\n",
    "    fwaa_list =fwaa.values.tolist()\n",
    "\n",
    "    fdaa = nlp_train_pos[['factors_did_attract_affiliate', 'factors_did_attract_affiliate_labels']]\n",
    "    fdaa = fdaa.explode('factors_did_attract_affiliate_labels', ignore_index=True)#\n",
    "    fdaa = fdaa[fdaa['factors_did_attract_affiliate_labels'] !='0']\n",
    "    fdaa_list = fdaa.values.tolist()\n",
    "\n",
    "    pos_list = fwaa_list + fdaa_list\n",
    "    pos_train = pd.DataFrame(pos_list)\n",
    "    pos_train.columns = ['text', 'labels']\n",
    "\n",
    "    #Negative Cleaning, creating the negative set and dropping non-responses\n",
    "    nlp_train_negative = (full_df[['changes_keep', 'changes_keep_labels', 'factors_would_deter_affiliate', 'factors_would_deter_affiliate_labels', 'drawbacks_affiliate', 'drawbacks_affiliate_labels', 'factors_prevent_satisfaction', 'factors_prevent_satisfaction_labels']]).iloc[0:num_labs]\n",
    "\n",
    "\n",
    "    ck = nlp_train_negative[['changes_keep', 'changes_keep_labels']]\n",
    "    ck = ck.explode('changes_keep_labels', ignore_index=True)\n",
    "    ck = ck[ck['changes_keep_labels'] !='0']\n",
    "    ck_list =ck.values.tolist()\n",
    "\n",
    "\n",
    "\n",
    "    fwda = nlp_train_negative[['factors_would_deter_affiliate', 'factors_would_deter_affiliate_labels']]\n",
    "    fwda = fwda.explode('factors_would_deter_affiliate_labels', ignore_index=True)\n",
    "    fwda = fwda[fwda['factors_would_deter_affiliate_labels'] !='0']\n",
    "    fwda_list =fwda.values.tolist()\n",
    "\n",
    "\n",
    "    da = nlp_train_negative[['drawbacks_affiliate', 'drawbacks_affiliate_labels']]\n",
    "    da = da.explode('drawbacks_affiliate_labels', ignore_index=True)\n",
    "    da = da[da['drawbacks_affiliate_labels'] !='0']\n",
    "    da_list =da.values.tolist()\n",
    "\n",
    "\n",
    "\n",
    "    fps = nlp_train_negative[['factors_prevent_satisfaction', 'factors_prevent_satisfaction_labels']]\n",
    "    fps = fps.explode('factors_prevent_satisfaction_labels', ignore_index=True)\n",
    "    fps = fps[fps['factors_prevent_satisfaction_labels'] !='0']\n",
    "    fps_list =fps.values.tolist()\n",
    "\n",
    "\n",
    "    neg_list = ck_list + fwda_list + da_list + fps_list\n",
    "    neg_train = pd.DataFrame(neg_list)\n",
    "    neg_train.columns = ['text', 'labels']\n",
    "\n",
    "    #Other Cleaning, creating the other set and dropping non-responses\n",
    "    nlp_train_other=(full_df[['factors_consider_during_eval', 'factors_consider_during_eval_labels', 'additional_comments', 'additional_comments_labels']]).iloc[0:num_labs]\n",
    "\n",
    "    fcde = nlp_train_other[['factors_consider_during_eval', 'factors_consider_during_eval_labels']]\n",
    "    fcde = fcde.explode('factors_consider_during_eval_labels', ignore_index=True)\n",
    "    fcde = fcde[fcde['factors_consider_during_eval_labels'] !='0']\n",
    "    fcde_list =fcde.values.tolist()\n",
    "\n",
    "    ac = nlp_train_other[['additional_comments', 'additional_comments_labels']]\n",
    "    ac = ac.explode('additional_comments_labels', ignore_index=True)\n",
    "    ac = ac[ac['additional_comments_labels'] !='0']\n",
    "    ac_list =ac.values.tolist()\n",
    "\n",
    "    other_list = ac_list + fcde_list\n",
    "    other_train = pd.DataFrame(other_list)\n",
    "    other_train.columns = ['text', 'labels']\n",
    "    \n",
    "\n",
    "    \n",
    "    return(pos_train, neg_train, other_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f711570",
   "metadata": {},
   "source": [
    "# Class Weight Dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fed1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights calculated based on the occurence of these factors in the training set\n",
    "pos_class_weights = {\n",
    "    0:1,\n",
    "    1: 267/42,\n",
    "    2:267/57,\n",
    "    3:267/60,\n",
    "    4:267/31,\n",
    "    5:267/10,\n",
    "    6:267/23,\n",
    "    7:267/9,\n",
    "    8:267/19,\n",
    "    9:267/13,\n",
    "    10:267/3    \n",
    "}\n",
    "\n",
    "neg_class_weights = {\n",
    "    0:1,\n",
    "    1: 821/101,\n",
    "    2:821/59,\n",
    "    3:821/91,\n",
    "    4:821/133,\n",
    "    5:821/65,\n",
    "    6:821/42,\n",
    "    7:821/102,\n",
    "    8:821/109,\n",
    "    9:821/26,\n",
    "    10: 821/26,\n",
    "    11: 821/38,\n",
    "    12: 821/12,\n",
    "    13: 821/9,\n",
    "    14: 821/8,\n",
    "}\n",
    "other_class_weights = {\n",
    "    0:1,\n",
    "    1: 678/69,\n",
    "    2: 678/71,\n",
    "    3: 678/32,\n",
    "    4: 678/90,\n",
    "    5: 678/70,\n",
    "    6: 678/92,\n",
    "    7: 678/41,\n",
    "    8: 678/56,\n",
    "    9: 678/98,\n",
    "    10: 678/18,\n",
    "    11: 678/35,\n",
    "    12: 678/6,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3486a5f",
   "metadata": {},
   "source": [
    "# Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00fbd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting each of our sets up into smaller, more usable sets\n",
    "pos_train, neg_train, other_train = cleaning_func(250, train)\n",
    "pos_val, neg_val, other_val = cleaning_func(50, val)\n",
    "pos_test, neg_test, other_test = cleaning_func(50, test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912d0c1a",
   "metadata": {},
   "source": [
    "# Model Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1856fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {}\n",
    "max_len = 100\n",
    "\n",
    "#creating and trianing a model in each category using our best model hyperparameters found in our search\n",
    "# makes a pickle file of each model's performance at accepted probability cutoffs from 0 to .99\n",
    "\n",
    "pos_1 = \"weightBERTADAM0.00001\"+ str(num_labs)\n",
    "mod_name = pos_1\n",
    "pos1_hist, pos1_x_val, pos1_y_val, pos1_mask_val, pos1_mod=train_mods(pos_train,pos_val, 11, 0.00001, 'ADAM', 'BERT', pos_class_weights)\n",
    "pos1_mod.save(mod_name)\n",
    "pos1_jac = model_testing(pos1_mod, pos1_x_val, pos1_y_val, pos1_mask_val, 11)\n",
    "pos1_jac.extend([0]*(max_len-len(pos1_jac)))\n",
    "\n",
    "pos_2 = \"weightDISTILBERTADAM0.00001\"+ str(num_labs)\n",
    "mod_name = pos_2\n",
    "pos2_hist, pos2_x_val, pos2_y_val, pos2_mask_val, pos2_mod=train_mods(pos_train,pos_val, 11, 0.00001, 'ADAM', 'DISTILBERT', pos_class_weights)\n",
    "pos2_mod.save(mod_name)\n",
    "pos2_jac = model_testing(pos2_mod, pos2_x_val, pos2_y_val, pos2_mask_val, 11)\n",
    "pos2_jac.extend([0]*(max_len-len(pos2_jac)))\n",
    "\n",
    "pos_dat = {pos_2:pos2_jac}\n",
    "pos_output = pd.DataFrame(pos_dat)\n",
    "outputs[\"Positive\"] = pos_output\n",
    "\n",
    "neg_1 = \"weightDISTILBERTRMSPROP0.00005\"+ str(num_labs)\n",
    "mod_name = neg_1\n",
    "neg1_hist, neg1_x_val, neg1_y_val, neg1_mask_val, neg1_mod=train_mods(neg_train,neg_val, 15, 0.00005, 'RMSPROP', 'DISTILBERT', neg_class_weights)\n",
    "neg1_mod.save(mod_name)\n",
    "neg1_jac = model_testing(neg1_mod, neg1_x_val, neg1_y_val, neg1_mask_val, 15)\n",
    "neg1_jac.extend([0]*(max_len-len(neg1_jac)))\n",
    "\n",
    "neg_2 = \"weightDISTILBERTRMSPROP0.00002\"+ str(num_labs)\n",
    "mod_name = neg_2\n",
    "neg2_hist, neg2_x_val, neg2_y_val, neg2_mask_val, neg2_mod=train_mods(neg_train,neg_val, 15, 0.00005, 'RMSPROP', 'DISTILBERT', neg_class_weights)\n",
    "neg2_mod.save(mod_name)\n",
    "neg2_jac = model_testing(neg2_mod, neg2_x_val, neg2_y_val, neg2_mask_val, 15)\n",
    "neg2_jac.extend([0]*(max_len-len(neg2_jac)))\n",
    "\n",
    "neg_dat = {neg_1:neg1_jac, neg_2:neg2_jac}\n",
    "neg_output = pd.DataFrame(neg_dat)\n",
    "\n",
    "outputs[\"Negative\"] = neg_output\n",
    "\n",
    "\n",
    "\n",
    "other_1 = \"weightBERTRMSPROP0.0001\"+ str(num_labs)\n",
    "mod_name = other_1\n",
    "other1_hist, other1_x_val, other1_y_val, other1_mask_val, other1_mod=train_mods(other_train,other_val, 13, 0.0001, 'RMSPROP', 'BERT', other_class_weights)\n",
    "other1_mod.save(mod_name)\n",
    "other1_jac = model_testing(other1_mod, other1_x_val, other1_y_val, other1_mask_val, 13)\n",
    "other1_jac.extend([0]*(max_len-len(other1_jac)))\n",
    "\n",
    "other_2 = \"weightBERTRMSPROP0.00005\"+ str(num_labs)\n",
    "mod_name = other_2\n",
    "other2_hist, other2_x_val, other2_y_val, other2_mask_val, other2_mod=train_mods(other_train,other_val, 13, 0.00005, 'RMSPROP', 'BERT', other_class_weights)\n",
    "other2_mod.save(mod_name)\n",
    "other2_jac = model_testing(other2_mod, other2_x_val, other2_y_val, other2_mask_val, 13)\n",
    "other2_jac.extend([0]*(max_len-len(other2_jac)))\n",
    "\n",
    "other_dat = {other_1:other1_jac, other_2:other2_jac}\n",
    "other_output = pd.DataFrame(other_dat)\n",
    "\n",
    "outputs[\"Other\"] = other_output\n",
    "\n",
    "\n",
    "\n",
    "pkl_name  = 'extended_results_weighted_pos_2' + str(num_labs) + '.pkl'\n",
    "json_name = 'model_histores' + str(num_labs) + '.json'\n",
    "with open(pkl_name, 'wb') as handle:\n",
    "    pickle.dump(outputs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004fb387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f91149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df63b7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
